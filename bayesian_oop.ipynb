{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 14 16:11:15 2020\n",
    "\n",
    "@author: Kshitij\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# use available dataset for classification\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 2/3, random_state = 1)\n",
    "\n",
    "''' We will be using lighgbm api without sklearn so we will provide the params\n",
    " in a dict format'''\n",
    "param = param = {'objective': 'binary', \n",
    "                 'learning_rate': 0.5,\n",
    "                 'reg_alpha': 0.5, \n",
    "                 'reg_lambda': 0.5}\n",
    "\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "# train the model\n",
    "model = lgb.train(param, lgb.Dataset(X_train, label = y_train))\n",
    "\n",
    "# make prediction on test dataset\n",
    "# unlike sklearn classifier the 'predict' method gives probability in lightgbm\n",
    "pred=model.predict(X_test)\n",
    "# we get the y_pred with threshold at 0.5\n",
    "y_pred = np.where(pred>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
      " 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9561752988047808"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "\n",
    "# see the f1 score: harmonic mean of precision and recall\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.30trial/s, best loss: 0.028112449799196693]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda_l1': 0.29726010349464893,\n",
       " 'lambda_l2': 0.42617329194394293,\n",
       " 'learning_rate': 0.1193862634945627}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperopt import  fmin, hp, tpe, Trials, STATUS_OK\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "    h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train), verbose_eval=False)\n",
    "    pred    = h_model.predict(X_test)\n",
    "    y_pred  = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
    "    f1sc = f1_score(y_test, y_pred)\n",
    "    loss = 1 - f1sc\n",
    "    \n",
    "    return {'loss': loss, 'status' : STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
    "    'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
    "    'learning_rate' : hp.loguniform('learning_rate', np.log(0.05), np.log(0.25)),\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'verbose': -1\n",
    "    }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# define the optimization function\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            trials= trials, \n",
    "            max_evals=100)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_l1': 0.29726010349464893,\n",
       " 'lambda_l2': 0.42617329194394293,\n",
       " 'learning_rate': 0.1193862634945627}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on best parameter results\n",
    "h_model = lgb.train(best, lgb.Dataset(X_train, label = y_train), verbose_eval=-1)\n",
    "\n",
    "# get the y_pred\n",
    "pred_h=h_model.predict(X_test)\n",
    "y_predh = list(map(lambda x: int(x), pred_h>0.5))\n",
    "\n",
    "f1_score(y_test, y_predh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlclass():\n",
    "    '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''\n",
    "    def __init__(self, x_train, y_train):\n",
    "        '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.\n",
    "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.train_set = lgb.Dataset(data=x_train, label=y_train)\n",
    "\n",
    "    def tuning(self, optim_type):\n",
    "        '''Method takes the optimization type and tunes the model'''\n",
    "        #call the optim_type: Hyperopt or Optuna\n",
    "        optimization = getattr(self, optim_type)\n",
    "        return optimization()\n",
    "  \n",
    "    def hyperopt_method(self):\n",
    "        # This method is called by tuning when user inputs 'hyperopt_method' while calling the tuning method\n",
    "    \n",
    "        #define the hyperopt space\n",
    "        space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
    "                 'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
    "                 'learning_rate' : hp.loguniform('learning_rate',\n",
    "                                                 np.log(0.05), np.log(0.25)),\n",
    "                 'objective' : 'binary',\n",
    "                 'verbose': -1\n",
    "                }\n",
    "        # define algorithm and trials inside the class\n",
    "        algo, trials= tpe.suggest, Trials()\n",
    "        \n",
    "        #Call the fmin from inside the class\n",
    "        best = fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=100)\n",
    "        \n",
    "        self.params = best\n",
    "        return best, trials\n",
    "    \n",
    "    def objective(self, params):\n",
    "        # same objective function with added self\n",
    "        h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))\n",
    "        pred=h_model.predict(X_test)\n",
    "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
    "        f1sc = f1_score(y_test, y_pred)\n",
    "        loss = 1 - f1sc\n",
    "        return {'loss': loss,'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obj = Mlclass(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.47trial/s, best loss: 0.02400000000000002]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'lambda_l1': 0.05048971454466432,\n",
       "  'lambda_l2': 0.37705506942576894,\n",
       "  'learning_rate': 0.1374934083843963},\n",
       " <hyperopt.base.Trials at 0x7f4beb5a8b70>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Obj.tuning('hyperopt_method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlclass():\n",
    "    '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''\n",
    "    def __init__(self, x_train, y_train):\n",
    "        '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.\n",
    "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.train_set = lgb.Dataset(data=x_train, label=y_train)\n",
    "\n",
    "    def tuning(self, optim_type):\n",
    "        '''Method takes the optimization type and tunes the model'''\n",
    "        #call the optim_type: Hyperopt or Optuna\n",
    "        optimization = getattr(self, optim_type)\n",
    "        return optimization()\n",
    "  \n",
    "    def hyperopt_method(self):\n",
    "        # This method is called by tuning when user inputs 'hyperopt_method' while calling the tuning method\n",
    "    \n",
    "        #define the hyperopt space\n",
    "        space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
    "                 'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
    "                 'learning_rate' : hp.loguniform('learning_rate',\n",
    "                                                 np.log(0.05), np.log(0.25)),\n",
    "                 'objective' : 'binary'}\n",
    "        # define algorithm and trials inside the class\n",
    "        algo, trials= tpe.suggest, Trials()\n",
    "        #Call the fmin from inside the class\n",
    "        best = fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=1000)\n",
    "        self.params = best\n",
    "        return best, trials\n",
    "    \n",
    "    def objective(self, params):\n",
    "        # same objective function with added self\n",
    "        h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))\n",
    "        pred=h_model.predict(X_test)\n",
    "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
    "        f1sc = f1_score(y_test, y_pred)\n",
    "        loss = 1 - f1sc\n",
    "        return {'loss': loss,'status' : STATUS_OK}\n",
    "    \n",
    "    def optuna_method(self):\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(optuna_obj, n_trials=1000)\n",
    "        self.params = study.best_params\n",
    "        return study\n",
    "    \n",
    "    def optuna_obj(self, trial):\n",
    "        '''Same optuna objective with parameters space inside the function for optuna optimization'''\n",
    "        params = {'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "                  'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "                  'learning_rate' : trial.suggest_loguniform('learning_rate', 0.05, 0.25)}\n",
    "        \n",
    "        o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))\n",
    "        pred=o_model.predict(X_test)\n",
    "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
    "        f1sc = f1_score(y_test, y_pred)\n",
    "        loss = 1 - f1sc\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"This function evaluates the model on best parameters\"\"\"\n",
    "        print(\"Model will be trained on the following parameters: \\n{}\".format(self.params))\n",
    "        #train the model with best parameters\n",
    "        self.gbm = lgb.train(self.params, self.train_set)\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # predict the values from x_test\n",
    "        pred = self.gbm.predict(x_test)\n",
    "        y_pred = np.where(pred>0.5,1,0)\n",
    "        #print confusion matrix\n",
    "        print(confusion_matrix(y_test,y_pred))\n",
    "        #print classification report\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
